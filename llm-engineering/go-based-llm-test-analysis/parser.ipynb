{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a343f814-3901-44eb-8320-50000a88273e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def parse_go_test_log(file_path: str):\n",
    "    tests = defaultdict(lambda: {\"status\": None, \"elapsed\": None, \"outputs\": []})\n",
    "\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                event = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                continue  # skip malformed lines\n",
    "\n",
    "            test_name = event.get(\"Test\")\n",
    "            action = event.get(\"Action\")\n",
    "            output = event.get(\"Output\")\n",
    "\n",
    "            if not test_name:\n",
    "                continue\n",
    "\n",
    "            # Capture outputs\n",
    "            if action == \"output\" and output:\n",
    "                tests[test_name][\"outputs\"].append(output.strip())\n",
    "\n",
    "            # Capture pass/fail status\n",
    "            if action in [\"fail\", \"pass\"]:\n",
    "                tests[test_name][\"status\"] = action\n",
    "                tests[test_name][\"elapsed\"] = event.get(\"Elapsed\", None)\n",
    "\n",
    "    # Keep only failing tests\n",
    "    failures = []\n",
    "    for test_name, data in tests.items():\n",
    "        if data[\"status\"] == \"fail\":\n",
    "            failures.append({\n",
    "                \"test_name\": test_name,\n",
    "                \"status\": data[\"status\"],\n",
    "                \"elapsed\": data[\"elapsed\"],\n",
    "                \"log\": \"\\n\".join(data[\"outputs\"])\n",
    "            })\n",
    "\n",
    "    return failures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a26b8da-bc8a-4ac2-8da2-046dbc0a8c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "failures = parse_go_test_log(\"test.log\")\n",
    "count=0\n",
    "for f in failures:\n",
    "    if f[\"status\"] == \"fail\":\n",
    "        count+=1\n",
    "        print(f\"\\nTest: {f['test_name']}\")\n",
    "        print(f\"Status: {f['status']}\")\n",
    "        print(f\"Log:\\n{f['log']}\")\n",
    "        print(\"******\\n\")\n",
    "\n",
    "print(\"Total Test failures:\",count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f602f451-153f-49d1-9b91-e36c7cfd4459",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are an expert Go developer and test automation engineer specializing in the Testify framework. \n",
    "\n",
    "When analyzing Go testify test failures, focus on:\n",
    "1. Go-specific issues: Goroutine leaks, race conditions, interface implementations, nil pointer dereferences\n",
    "2. Testify patterns: Suite setup/teardown issues, assertion failures, mock problems  \n",
    "3. Common Go testing pitfalls: Table-driven test issues, test isolation problems, timing-sensitive tests\n",
    "\n",
    "Your output **must be a valid JSON object** with this structure:\n",
    "\n",
    "{\n",
    "  \"Testname\": \"<name of the test>\",\n",
    "  \"RootCause\": \"<What caused the failure in 1-2 sentences>\",\n",
    "  \"Fix\": \"<Concrete solution in 1-2 sentences>\",\n",
    "  \"Prevention\": \"<How to avoid this in the future>\"\n",
    "}\n",
    "\n",
    "**Rules**:\n",
    "- Do NOT include any extra text outside this JSON.\n",
    "- Keep the response under 200 words.\n",
    "- Focus on actionable insights.\n",
    "- Use valid JSON syntax with double quotes.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c8c5668-0e19-4d8d-a44d-720eafcc90ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_prompt_for(test_failure: dict) -> str:\n",
    "    \"\"\"\n",
    "    Build the user prompt for LLM analysis using a concise format.\n",
    "\n",
    "    Args:\n",
    "        test_failure (dict): Single test failure dict containing 'test_name' and 'log'\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted user prompt string\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "Provide concise root cause analysis focusing on Go/testify specifics.\n",
    "\n",
    "Test Name: {test_failure['test_name']}\n",
    "Log:\n",
    "{test_failure['log']}\n",
    "\n",
    "Return only JSON like this example:\n",
    "{{\n",
    "  \"TestName\": \"TEST_XCP\",\n",
    "  \"RootCause\": \"Database timeout\",\n",
    "  \"GoTestifyIssue\": \"Nil pointer dereference\",\n",
    "  \"Fix\": \"Increase DB pool size or retry\",\n",
    "  \"Prevention\": \"Add error handling and unit tests\"\n",
    "}}\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24d9d8b3-d379-4918-9be9-f4405872bbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "def llama_summarize(messages: List[Dict]) -> Dict:\n",
    "    \"\"\"\n",
    "    Calls the Llama model via OpenAI-compatible API.\n",
    "    Returns structured JSON content.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama3.2\",\n",
    "        messages=messages,\n",
    "        response_format={\"type\": \"json\"}  # ensures JSON output\n",
    "    )\n",
    "    return json.loads(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Single failure analysis\n",
    "# ----------------------\n",
    "def analyze_failure(test_failure: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze a single test failure using Llama, with safe JSON parsing.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(test_failure)}\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        response_text = llama_summarize(messages)  # Calls Llama client\n",
    "\n",
    "        try:\n",
    "            analysis = json.loads(response_text)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Warning: LLM returned invalid JSON for {test_failure['test_name']}\")\n",
    "            analysis = {\n",
    "                \"TestName\": test_failure[\"test_name\"],\n",
    "                \"RootCause\": response_text.strip(),\n",
    "                \"GoTestifyIssue\": \"\",\n",
    "                \"Fix\": \"\",\n",
    "                \"Prevention\": \"\"\n",
    "            }\n",
    "\n",
    "        # Ensure TestName is included\n",
    "        if \"TestName\" not in analysis:\n",
    "            analysis[\"TestName\"] = test_failure[\"test_name\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        analysis = {\n",
    "            \"TestName\": test_failure[\"test_name\"],\n",
    "            \"RootCause\": \"Error analyzing with LLM\",\n",
    "            \"GoTestifyIssue\": \"\",\n",
    "            \"Fix\": \"\",\n",
    "            \"Prevention\": \"\",\n",
    "            \"Error\": str(e)\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"test_name\": test_failure[\"test_name\"],\n",
    "        \"elapsed\": test_failure.get(\"elapsed\"),\n",
    "        \"analysis\": analysis\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc7ae05a-0315-4e4d-a755-40d22b65fc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "def analyze_failures(failures: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Analyze failed test logs with Llama and return structured JSON per failure.\n",
    "    \n",
    "    Args:\n",
    "        failures (list): List of failed test dicts from parse_go_test_log\n",
    "    \n",
    "    Returns:\n",
    "        list: Structured analysis results per failure\n",
    "    \"\"\"\n",
    "    analyses = []\n",
    "    failed_tests = [f for f in failures if f[\"status\"] == \"fail\"]\n",
    "    total = len(failed_tests)\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Starting analysis for {total} failed tests...\\n\")\n",
    "\n",
    "    for idx, f in enumerate(failed_tests, start=1):\n",
    "        test_name = f[\"test_name\"]\n",
    "        print(f\"[{idx}/{total}] Analyzing test: {test_name} ...\")\n",
    "        test_start = time.time()\n",
    "\n",
    "        # Build messages for Llama\n",
    "        user_msg = user_prompt_for(f)\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_msg}\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            # Call Llama via client wrapper\n",
    "            analysis = llama_summarize(messages)\n",
    "\n",
    "            # Include TestName explicitly for clarity\n",
    "            structured_analysis = {\n",
    "                \"TestName\": test_name,\n",
    "                **analysis\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            structured_analysis = {\n",
    "                \"TestName\": test_name,\n",
    "                \"RootCause\": \"Error analyzing with LLM\",\n",
    "                \"GoTestifyIssue\": \"\",\n",
    "                \"Fix\": \"\",\n",
    "                \"Prevention\": \"\",\n",
    "                \"Error\": str(e)\n",
    "            }\n",
    "\n",
    "        analyses.append({\n",
    "            \"test_name\": test_name,\n",
    "            \"elapsed\": f.get(\"elapsed\"),\n",
    "            \"analysis\": structured_analysis\n",
    "        })\n",
    "\n",
    "        # ETA calculation\n",
    "        test_elapsed = time.time() - test_start\n",
    "        avg_time = (time.time() - start_time) / idx\n",
    "        remaining = total - idx\n",
    "        eta = remaining * avg_time\n",
    "\n",
    "        # Console feedback with LLM output\n",
    "        print(f\"[{idx}/{total}] Completed analysis for: {test_name} \"\n",
    "              f\"(took {test_elapsed:.2f}s, ETA: {eta:.2f}s)\")\n",
    "        print(f\"LLM Analysis:\\n{json.dumps(structured_analysis, indent=2)}\\n\")\n",
    "\n",
    "    total_elapsed = time.time() - start_time\n",
    "    print(f\"Analysis complete for all {total} failures in {total_elapsed:.2f}s.\\n\")\n",
    "\n",
    "    return analyses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe8cdc8e-b56b-420e-84ae-1689ecaeb4fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis for 7 failed tests...\n",
      "\n",
      "[1/7] Analyzing test: TestEntitySanityTestSuite ...\n",
      "[1/7] Completed analysis for: TestEntitySanityTestSuite (took 2.61s, ETA: 15.63s)\n",
      "LLM Analysis:\n",
      "{\n",
      "  \"TestName\": \"TestEntitySanityTestSuite\",\n",
      "  \"RootCause\": \"Error analyzing with LLM\",\n",
      "  \"GoTestifyIssue\": \"\",\n",
      "  \"Fix\": \"\",\n",
      "  \"Prevention\": \"\",\n",
      "  \"Error\": \"Expecting value: line 1 column 1 (char 0)\"\n",
      "}\n",
      "\n",
      "[2/7] Analyzing test: TestEntitySanityTestSuite/Test_XCP_Tagging_Entity_Idempotency_Validation_0021 ...\n",
      "[2/7] Completed analysis for: TestEntitySanityTestSuite/Test_XCP_Tagging_Entity_Idempotency_Validation_0021 (took 2.68s, ETA: 13.21s)\n",
      "LLM Analysis:\n",
      "{\n",
      "  \"TestName\": \"TestEntitySanityTestSuite/Test_XCP.Tagging_Entity_Idempotency_Validation_0021\",\n",
      "  \"RootCause\": \"Potential nil pointer dereference due to mismatched expected and actual response schema.\",\n",
      "  \"GoTestifyIssue\": \"Missing assertion for nil check in 'TaggingEntity'\",\n",
      "  \"Fix\": \"Add assertions for nil checks, e.g. `if taggingEntity == nil { t.Errorf('expected TaggingEntity', %v)', taggingEntity)`,\",\n",
      "  \"Prevention\": \"Implement comprehensive schema validation and ensure consistent response formats\"\n",
      "}\n",
      "\n",
      "[3/7] Analyzing test: TestEntitySanityTestSuite/Test_XCP_Tagging_Entity_Idempotency_Validation_0021/Create_Duplicate_Tag ...\n",
      "[3/7] Completed analysis for: TestEntitySanityTestSuite/Test_XCP_Tagging_Entity_Idempotency_Validation_0021/Create_Duplicate_Tag (took 3.37s, ETA: 11.54s)\n",
      "LLM Analysis:\n",
      "{\n",
      "  \"TestName\": \"Create_Duplicate_Tag\",\n",
      "  \"RootCause\": \"Mock API response failure to indicate duplicate entity existence\",\n",
      "  \"GoTestifyIssue\": \"Assertion in testify Suite should expect map mismatch instead of just a generic unprocessable entity error code\",\n",
      "  \"Fix\": \"Update assertion to exact error message or use a deeper equality check\",\n",
      "  \"Prevention\": \"Use more specific assertions and verify expected errors are returned\"\n",
      "}\n",
      "\n",
      "[4/7] Analyzing test: TestEntitySanityTestSuite/Test_XCP_Tagging_Entity_Idempotency_Validation_0021/Create_Duplicate_Entity_with_Same_Name_Different_Description ...\n",
      "[4/7] Completed analysis for: TestEntitySanityTestSuite/Test_XCP_Tagging_Entity_Idempotency_Validation_0021/Create_Duplicate_Entity_with_Same_Name_Different_Description (took 4.62s, ETA: 9.96s)\n",
      "LLM Analysis:\n",
      "{\n",
      "  \"TestName\": \"CREATE_Duplicate_Entity_with_Same_Name_Different_Description\",\n",
      "  \"RootCause\": \"The API failed with a status code of 422 Unprocessable Entity indicating that there was an issue with the duplicate entity creation, specifically with the naming convention. This is due to the nil pointer dereference in testify\",\n",
      "  \"Fix\": \"Add assertions to verify that the generated data was correct and contains the expected error message\",\n",
      "  \"Prevention\": \"Double-check API documentation and thoroughly test for edge cases\"\n",
      "}\n",
      "\n",
      "[5/7] Analyzing test: TestEntitySanityTestSuite/Test_XCP_Tagging_Entity_Idempotency_for_Bulk_Creation_Validation_0022 ...\n",
      "[5/7] Completed analysis for: TestEntitySanityTestSuite/Test_XCP_Tagging_Entity_Idempotency_for_Bulk_Creation_Validation_0022 (took 2.18s, ETA: 6.19s)\n",
      "LLM Analysis:\n",
      "{\n",
      "  \"TestName\": \"TestEntitySanityTestSuite/Test_XCP_Tagging_Entity_Idempotency_for_Bulk_Creation_Validation_0022\",\n",
      "  \"RootCause\": \"Error analyzing with LLM\",\n",
      "  \"GoTestifyIssue\": \"\",\n",
      "  \"Fix\": \"\",\n",
      "  \"Prevention\": \"\",\n",
      "  \"Error\": \"Expecting value: line 1 column 1 (char 0)\"\n",
      "}\n",
      "\n",
      "[6/7] Analyzing test: TestEntitySanityTestSuite/Test_XCP_Tagging_Entity_Idempotency_for_Bulk_Creation_Validation_0022/Create_Duplicate_Entity ...\n",
      "[6/7] Completed analysis for: TestEntitySanityTestSuite/Test_XCP_Tagging_Entity_Idempotency_for_Bulk_Creation_Validation_0022/Create_Duplicate_Entity (took 3.83s, ETA: 3.22s)\n",
      "LLM Analysis:\n",
      "{\n",
      "  \"TestName\": \"Create_Duplicate_Entity\",\n",
      "  \"RootCause\": \"Misinterpreted response from the server, expecting \\\"Entity created successfully\\\", but got \\\"Entity with given name exists\\\"\",\n",
      "  \"GoTestifyIssue\": \"Nil pointer dereference due to mismatched expectation and actual message strings\",\n",
      "  \"Fix\": \"Update expectation to match the actual message string,\\\"Entity with given name exists\\\",\\\"\\\"\",\n",
      "  \"Prevention\": \"Review assertion expectations against server responses, use expected message exact matches\"\n",
      "}\n",
      "\n",
      "[7/7] Analyzing test: TestEntitySanityTestSuite/Test_XCP_Tagging_Entity_Idempotency_for_Bulk_Creation_Validation_0022/Create_Duplicate_Entity_with_Same_Name_Different_Description ...\n",
      "[7/7] Completed analysis for: TestEntitySanityTestSuite/Test_XCP_Tagging_Entity_Idempotency_for_Bulk_Creation_Validation_0022/Create_Duplicate_Entity_with_Same_Name_Different_Description (took 4.64s, ETA: 0.00s)\n",
      "LLM Analysis:\n",
      "{\n",
      "  \"TestName\": \"Create_Duplicate_Entity_with_Same_Name_Different_Description\",\n",
      "  \"RootCause\": \"The request data does not match the database data, and no error message is provided in the API response.\",\n",
      "  \"GoTestifyIssue\": \"Nil pointer dereference\",\n",
      "  \"Fix\": \"Update the test to include the expected error message in the assertion.\",\n",
      "  \"Prevention\": \"Verify that error messages are correctly returned by the API\"\n",
      "}\n",
      "\n",
      "Analysis complete for all 7 failures in 23.94s.\n",
      "\n",
      "\n",
      "===============================\n",
      "Test: TestEntitySanityTestSuite\n",
      "Root Cause: Error analyzing with LLM\n",
      "Go/Testify Issue: \n",
      "Fix: \n",
      "Prevention: \n",
      "⚠️ LLM Error: Expecting value: line 1 column 1 (char 0)\n",
      "===============================\n",
      "\n",
      "\n",
      "===============================\n",
      "Test: TestEntitySanityTestSuite/Test_XCP.Tagging_Entity_Idempotency_Validation_0021\n",
      "Root Cause: Potential nil pointer dereference due to mismatched expected and actual response schema.\n",
      "Go/Testify Issue: Missing assertion for nil check in 'TaggingEntity'\n",
      "Fix: Add assertions for nil checks, e.g. `if taggingEntity == nil { t.Errorf('expected TaggingEntity', %v)', taggingEntity)`,\n",
      "Prevention: Implement comprehensive schema validation and ensure consistent response formats\n",
      "===============================\n",
      "\n",
      "\n",
      "===============================\n",
      "Test: Create_Duplicate_Tag\n",
      "Root Cause: Mock API response failure to indicate duplicate entity existence\n",
      "Go/Testify Issue: Assertion in testify Suite should expect map mismatch instead of just a generic unprocessable entity error code\n",
      "Fix: Update assertion to exact error message or use a deeper equality check\n",
      "Prevention: Use more specific assertions and verify expected errors are returned\n",
      "===============================\n",
      "\n",
      "\n",
      "===============================\n",
      "Test: CREATE_Duplicate_Entity_with_Same_Name_Different_Description\n",
      "Root Cause: The API failed with a status code of 422 Unprocessable Entity indicating that there was an issue with the duplicate entity creation, specifically with the naming convention. This is due to the nil pointer dereference in testify\n",
      "Go/Testify Issue: \n",
      "Fix: Add assertions to verify that the generated data was correct and contains the expected error message\n",
      "Prevention: Double-check API documentation and thoroughly test for edge cases\n",
      "===============================\n",
      "\n",
      "\n",
      "===============================\n",
      "Test: TestEntitySanityTestSuite/Test_XCP_Tagging_Entity_Idempotency_for_Bulk_Creation_Validation_0022\n",
      "Root Cause: Error analyzing with LLM\n",
      "Go/Testify Issue: \n",
      "Fix: \n",
      "Prevention: \n",
      "⚠️ LLM Error: Expecting value: line 1 column 1 (char 0)\n",
      "===============================\n",
      "\n",
      "\n",
      "===============================\n",
      "Test: Create_Duplicate_Entity\n",
      "Root Cause: Misinterpreted response from the server, expecting \"Entity created successfully\", but got \"Entity with given name exists\"\n",
      "Go/Testify Issue: Nil pointer dereference due to mismatched expectation and actual message strings\n",
      "Fix: Update expectation to match the actual message string,\"Entity with given name exists\",\"\"\n",
      "Prevention: Review assertion expectations against server responses, use expected message exact matches\n",
      "===============================\n",
      "\n",
      "\n",
      "===============================\n",
      "Test: Create_Duplicate_Entity_with_Same_Name_Different_Description\n",
      "Root Cause: The request data does not match the database data, and no error message is provided in the API response.\n",
      "Go/Testify Issue: Nil pointer dereference\n",
      "Fix: Update the test to include the expected error message in the assertion.\n",
      "Prevention: Verify that error messages are correctly returned by the API\n",
      "===============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main(log_file: str = \"test.log\"):\n",
    "    \"\"\"\n",
    "    Main function to parse Go test logs, analyze failures with Llama,\n",
    "    and display structured results.\n",
    "    \"\"\"\n",
    "    # Step 1: Parse the Go test log\n",
    "    failures = parse_go_test_log(log_file)\n",
    "\n",
    "    # Step 2: Filter only failures\n",
    "    failed_tests = [f for f in failures if f.get(\"status\") == \"fail\"]\n",
    "\n",
    "    if not failed_tests:\n",
    "        print(\"✅ No test failures found.\")\n",
    "        return\n",
    "\n",
    "    # # Optional: print raw failed logs for debug\n",
    "    # for f in failed_tests:\n",
    "    #     print(f\"\\nTest: {f['test_name']}\")\n",
    "    #     print(f\"Elapsed: {f.get('elapsed')}\")\n",
    "    #     print(f\"Log:\\n{f['log']}\")\n",
    "    #     print(\"******\\n\")\n",
    "\n",
    "    # Step 3: Analyze failures with LLM\n",
    "    analyses = analyze_failures(failed_tests)\n",
    "\n",
    "    # Step 4: Print structured analysis\n",
    "    for analysis in analyses:\n",
    "        a = analysis[\"analysis\"]\n",
    "        print(\"\\n===============================\")\n",
    "        print(f\"Test: {a.get('TestName', analysis['test_name'])}\")\n",
    "        print(f\"Root Cause: {a.get('RootCause','')}\")\n",
    "        print(f\"Go/Testify Issue: {a.get('GoTestifyIssue','')}\")\n",
    "        print(f\"Fix: {a.get('Fix','')}\")\n",
    "        print(f\"Prevention: {a.get('Prevention','')}\")\n",
    "        if \"Error\" in a:\n",
    "            print(f\"⚠️ LLM Error: {a['Error']}\")\n",
    "        print(\"===============================\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de93a967-9aff-4918-afc0-07d232b5d42d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
